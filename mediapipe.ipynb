{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526d4cee-4858-4c38-8564-98b48a8f1ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (24.3.7)\n",
      "Requirement already satisfied: jax in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (3.8.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\jaseem\\appdata\\roaming\\python\\python311\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45631f73-bdf7-430b-8f36-58cdaba43f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462505c-8839-4080-b610-87a4b728f781",
   "metadata": {},
   "source": [
    "##MediaPipe is an open-source framework developed by Google that offers customizable machine learning (ML) pipelines to process media data such as images, video, and audio. It provides a wide range of pre-trained ML models and components to perform various tasks, including:\n",
    "\n",
    "Pose Estimation: Detecting and tracking human body poses in images and video.\n",
    "Hand Tracking: Identifying and tracking hands in images and video.\n",
    "Object Detection: Detecting and tracking objects in images and video streams.\n",
    "Face Detection: Detecting and tracking faces in images and video.\n",
    "Face Mesh: Estimating facial landmarks in real-time.\n",
    "Holistic: Combining multiple components like face detection, pose estimation, and hand tracking to create holistic pipelines.\n",
    "Selfie Segmentation: Segmenting a person's image from the background in real-time.\n",
    "Hair Segmentation: Segmenting hair from images and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d99b9d1-b55a-413d-afda-59242ec4f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_facedetection = mp.solutions.face_detection\n",
    "mp_drawings = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b582c-448e-409b-ae2a-fefcc28ff0da",
   "metadata": {},
   "source": [
    "1st line imports face detection module from mediapipe library and assingns it tothe variable mp_facedetection.\n",
    "this provides fumctionalities for detection gaces in images or videos.\n",
    "2nd line imports the drawing utility modules from mediapipe librry and assign it to variable mp_drawings\n",
    "this module contain functions for drawing annotations such as bounding boxes or landmarks on image or video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9420027-0f8f-47b9-a80a-e44168134611",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection = mp_facedetection.FaceDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c32d2-f6dd-441e-9eb8-d8147fa18621",
   "metadata": {},
   "source": [
    "The FaceDetection class in the Mediapipe library utilizes a pre-trained machine learning model to detect faces in images or video frames. This model has been trained on a large dataset of annotated images, allowing it to leam features and patterns assoclated with human faces. creating an instance of the face detection model from the Mediapipe library. The line face_detection=mp_facedetection.FaceDetection) initializes an instance of the FaceDetection class,\r\n",
    "This instance of the FaceDetection class will allow to perform face detection on images or video streams using the functionality provided by the Mediapipe llbrary, then use this face_detection object to detect faces in images or video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2121567-7812-4a09-b769-8941a7f4d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaseem\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "while True:\n",
    "     suc,img = video.read()\n",
    "     img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "     result = face_detection.process(img)\n",
    "     img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "     if result.detections:\n",
    "         for det in result.detections:\n",
    "\n",
    "             mp_drawings.draw_detection(img,det)\n",
    "     cv2.imshow('FACE',img)\n",
    "     if cv2.waitKey(1) & 0XFF==ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "     \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05f6041-829f-4b92-b637-c90d9eca74b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[label_id: 0\n",
       " score: 0.840784669\n",
       " location_data {\n",
       "   format: RELATIVE_BOUNDING_BOX\n",
       "   relative_bounding_box {\n",
       "     xmin: 0.450166106\n",
       "     ymin: 0.553373873\n",
       "     width: 0.366528213\n",
       "     height: 0.488668621\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.576532543\n",
       "     y: 0.715630949\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.731013238\n",
       "     y: 0.709518492\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.675069213\n",
       "     y: 0.851798594\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.669383228\n",
       "     y: 0.934106886\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.46329\n",
       "     y: 0.728914917\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.785996258\n",
       "     y: 0.713035762\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b9f77-031b-40bb-89e1-9a997abda3cf",
   "metadata": {},
   "source": [
    "face_detection process this code would typically detect faces in the image\n",
    "\n",
    "and store the result in the variable result\n",
    "\n",
    "Reading a Frame: suc, img = video.read() captures a frame from the video feed.\n",
    "\n",
    "Color Conversion (BGR to RGB): img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) converts the image from BGR (OpenCV's default) to RGB (required by MediaPipe).\n",
    "\n",
    "Face Detection: result = face_detection.process(img) processes the image to detect faces.\n",
    "Face Detection: result = face_detection.process(img) processes the image to detect faces.\n",
    "\n",
    "Color Conversion (RGB to BGR): img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) converts the image back to BGR for display with OpenCV.\n",
    "\n",
    "Drawing Face Detections: If faces are detected (if result.detections:), a loop iterates through each detected face.\n",
    "\n",
    "mp_drawings.draw_detection(img, det) draws the detected face on the image.\n",
    "Displaying the Image: cv2.imshow('FACE', img) displays the image with the detected faces.\n",
    "\n",
    "Exit Condition: if cv2.waitKey(1) & 0xFF == ord('q'): breaks the loop and exits if the 'q' key is pressed.\n",
    "cv2.waitKey(1): Waits for 1 millisecond for a key event.\n",
    "cv2.waitKey(1) & 0xFF: The bitwise AND operation ensures the result is within the ASCII range.\n",
    "== ord('q'): Compares the result to the ASCII value of 'q' to check if 'q' was pressed.\n",
    "If 'q' is pressed, the loop breaks,ending the video capture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
